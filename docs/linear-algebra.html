---
layout: MathNotes
title: Linear Algebra
subtitle: Vector Spaces, Linear Transformations, and Matrix Theory
---

<div class="section">
<h2>üìä Vector Spaces</h2>

<div class="theory-box">
<h4>Axiomatic Definition</h4>
<p>A vector space V over a field F (typically ‚Ñù or ‚ÑÇ) is a set equipped with two operations:</p>
<ul>
<li><strong>Vector addition:</strong> V √ó V ‚Üí V, written as u + v</li>
<li><strong>Scalar multiplication:</strong> F √ó V ‚Üí V, written as c¬∑v</li>
</ul>
<p>satisfying 10 axioms: associativity, commutativity of addition, additive identity (zero vector), additive inverses, 
multiplicative identity, and distributive properties.</p>
</div>

<div class="component-grid">
<div class="component-card">
<h4>Standard Examples</h4>
<ul>
<li><strong>‚Ñù‚Åø:</strong> n-tuples of real numbers with componentwise operations</li>
<li><strong>Matrices M_{m√ón}:</strong> All m√ón matrices with matrix addition and scalar multiplication</li>
<li><strong>Polynomials P_n:</strong> Polynomials of degree ‚â§ n with standard operations</li>
<li><strong>Function Spaces C[a,b]:</strong> Continuous functions on [a,b] with pointwise operations</li>
</ul>
</div>

<div class="component-card">
<h4>Subspaces</h4>
<p>A subset W ‚äÜ V is a subspace if:</p>
<ol>
<li>0 ‚àà W (contains zero vector)</li>
<li>Closed under addition: u, v ‚àà W ‚üπ u + v ‚àà W</li>
<li>Closed under scalar multiplication: v ‚àà W, c ‚àà F ‚üπ c¬∑v ‚àà W</li>
</ol>
<p><strong>Example:</strong> Solution space of homogeneous system Ax = 0 forms a subspace of ‚Ñù‚Åø.</p>
</div>
</div>
</div>

<div class="section">
<h2>üéØ Linear Independence and Bases</h2>

<div class="theory-box">
<h4>Linear Independence</h4>
<p>Vectors v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ are <strong>linearly independent</strong> if the equation:</p>
<div class="formula">c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çñv‚Çñ = 0</div>
<p>implies c‚ÇÅ = c‚ÇÇ = ... = c‚Çñ = 0. Otherwise, they are <strong>linearly dependent</strong>.</p>
<p><strong>Geometric Intuition:</strong> In ‚Ñù¬≥, three vectors are independent if they don't lie in the same plane through the origin.</p>
</div>

<div class="theory-box">
<h4>Basis and Dimension</h4>
<p>A <strong>basis</strong> for vector space V is a linearly independent spanning set. 
Key theorem: All bases of a finite-dimensional vector space have the same number of vectors, called the <strong>dimension</strong>.</p>
<div class="formula">dim(‚Ñù‚Åø) = n, dim(P_n) = n+1, dim(M_{m√ón}) = mn</div>
</div>

<table>
<tr>
<th>Vector Space</th>
<th>Standard Basis</th>
<th>Dimension</th>
</tr>
<tr>
<td>‚Ñù¬≥</td>
<td>{(1,0,0), (0,1,0), (0,0,1)}</td>
<td>3</td>
</tr>
<tr>
<td>P‚ÇÇ (polynomials deg ‚â§ 2)</td>
<td>{1, x, x¬≤}</td>
<td>3</td>
</tr>
<tr>
<td>M‚ÇÇ√ó‚ÇÇ (2√ó2 matrices)</td>
<td>Four matrices with single 1 entry</td>
<td>4</td>
</tr>
</table>
</div>

<div class="section">
<h2>‚ö° Matrix Operations</h2>

<div class="component-grid">
<div class="component-card">
<h4>Matrix Multiplication</h4>
<p>For A (m√ón) and B (n√óp):</p>
<div class="formula">(AB)·µ¢‚±º = Œ£‚Çñ a·µ¢‚Çñb‚Çñ‚±º</div>
<p><strong>Not commutative:</strong> Generally AB ‚â† BA<br>
<strong>Associative:</strong> (AB)C = A(BC)<br>
<strong>Distributive:</strong> A(B + C) = AB + AC</p>
</div>

<div class="component-card">
<h4>Transpose Properties</h4>
<div class="formula">
(A^T)^T = A<br>
(A + B)^T = A^T + B^T<br>
(AB)^T = B^T A^T<br>
(cA)^T = c A^T
</div>
<p><strong>Symmetric matrices:</strong> A = A^T<br>
<strong>Skew-symmetric:</strong> A = -A^T</p>
</div>

<div class="component-card">
<h4>Determinant</h4>
<p>For 2√ó2 matrix:</p>
<div class="formula">det([a b; c d]) = ad - bc</div>
<p>For n√ón matrix, use cofactor expansion along any row or column.</p>
<p><strong>Properties:</strong><br>
det(AB) = det(A)det(B)<br>
det(A^T) = det(A)<br>
det(cA) = c^n det(A) for n√ón matrix</p>
</div>

<div class="component-card">
<h4>Matrix Inverse</h4>
<p>A is invertible iff det(A) ‚â† 0:</p>
<div class="formula">A A^(-1) = A^(-1) A = I</div>
<p><strong>Properties:</strong><br>
(A^(-1))^(-1) = A<br>
(AB)^(-1) = B^(-1) A^(-1)<br>
(A^T)^(-1) = (A^(-1))^T</p>
</div>
</div>
</div>

<div class="section">
<h2>üîÑ Linear Transformations</h2>

<div class="theory-box">
<h4>Definition</h4>
<p>A function T: V ‚Üí W between vector spaces is a <strong>linear transformation</strong> if:</p>
<ol>
<li>T(u + v) = T(u) + T(v) for all u, v ‚àà V</li>
<li>T(cv) = cT(v) for all v ‚àà V and scalars c</li>
</ol>
<p><strong>Matrix Representation:</strong> Every linear transformation T: ‚Ñù‚Åø ‚Üí ‚Ñù·µê can be represented by an m√ón matrix A where T(x) = Ax.</p>
</div>

<div class="component-grid">
<div class="component-card">
<h4>Kernel (Null Space)</h4>
<div class="formula">ker(T) = {v ‚àà V : T(v) = 0}</div>
<p>Always a subspace of V. For matrix A:</p>
<div class="formula">Null(A) = {x : Ax = 0}</div>
<p><strong>Dimension:</strong> nullity(A) = n - rank(A)</p>
</div>

<div class="component-card">
<h4>Image (Range/Column Space)</h4>
<div class="formula">Im(T) = {T(v) : v ‚àà V}</div>
<p>Always a subspace of W. For matrix A:</p>
<div class="formula">Col(A) = span of column vectors</div>
<p><strong>Dimension:</strong> rank(A) = dim(Col(A))</p>
</div>

<div class="component-card">
<h4>Rank-Nullity Theorem</h4>
<div class="formula">dim(V) = rank(T) + nullity(T)</div>
<p>For an m√ón matrix A:</p>
<div class="formula">n = rank(A) + nullity(A)</div>
<p><strong>Interpretation:</strong> Dimensions of domain = dimensions mapped to range + dimensions collapsed to zero.</p>
</div>
</div>
</div>

<div class="section">
<h2>üé® Eigenvalues and Eigenvectors</h2>

<div class="theory-box">
<h4>Fundamental Definitions</h4>
<p>For square matrix A, a nonzero vector v is an <strong>eigenvector</strong> with <strong>eigenvalue</strong> Œª if:</p>
<div class="formula">Av = Œªv</div>
<p>This means v is scaled by Œª under transformation A, without changing direction.</p>
<p><strong>Characteristic Equation:</strong> det(A - ŒªI) = 0</p>
<p>The roots of this polynomial are the eigenvalues of A.</p>
</div>

<div class="component-grid">
<div class="component-card">
<h4>Eigenspace</h4>
<p>For eigenvalue Œª:</p>
<div class="formula">E_Œª = {v : Av = Œªv} = Null(A - ŒªI)</div>
<p>The eigenspace is the subspace of all eigenvectors with eigenvalue Œª, plus the zero vector.</p>
</div>

<div class="component-card">
<h4>Diagonalization</h4>
<p>A is diagonalizable if:</p>
<div class="formula">A = PDP^(-1)</div>
<p>where D is diagonal with eigenvalues and P has corresponding eigenvectors as columns.</p>
<p><strong>Criterion:</strong> A is diagonalizable iff it has n linearly independent eigenvectors.</p>
</div>

<div class="component-card">
<h4>Power Computation</h4>
<p>If A = PDP^(-1), then:</p>
<div class="formula">A^k = PD^k P^(-1)</div>
<p>Computing D^k is trivial (raise diagonal entries to k-th power), making large matrix powers efficient.</p>
</div>

<div class="component-card">
<h4>Spectral Theorem</h4>
<p>A real symmetric matrix A has:</p>
<ul>
<li>All real eigenvalues</li>
<li>Orthogonal eigenvectors</li>
<li>Orthogonal diagonalization: A = QŒõQ^T</li>
</ul>
<p>where Q is orthogonal (Q^T Q = I) and Œõ is diagonal.</p>
</div>
</div>
</div>

<div class="section">
<h2>üìè Inner Products and Orthogonality</h2>

<div class="theory-box">
<h4>Inner Product Spaces</h4>
<p>An inner product on vector space V is a function ‚ü®¬∑,¬∑‚ü©: V √ó V ‚Üí ‚Ñù satisfying:</p>
<ul>
<li>Linearity in first argument</li>
<li>Symmetry: ‚ü®u,v‚ü© = ‚ü®v,u‚ü©</li>
<li>Positive definiteness: ‚ü®v,v‚ü© ‚â• 0, with equality iff v = 0</li>
</ul>
<p><strong>Standard inner product on ‚Ñù‚Åø:</strong> ‚ü®x,y‚ü© = x¬∑y = Œ£ x·µ¢y·µ¢</p>
</div>

<div class="component-grid">
<div class="component-card">
<h4>Norm and Distance</h4>
<div class="formula">
‚Äñv‚Äñ = ‚àö‚ü®v,v‚ü©<br>
d(u,v) = ‚Äñu - v‚Äñ
</div>
<p><strong>Cauchy-Schwarz Inequality:</strong></p>
<div class="formula">|‚ü®u,v‚ü©| ‚â§ ‚Äñu‚Äñ¬∑‚Äñv‚Äñ</div>
<p>Equality holds iff u and v are linearly dependent.</p>
</div>

<div class="component-card">
<h4>Orthogonality</h4>
<p>Vectors u and v are <strong>orthogonal</strong> if:</p>
<div class="formula">‚ü®u,v‚ü© = 0</div>
<p><strong>Orthonormal basis:</strong> Basis {v‚ÇÅ,...,v‚Çô} with ‚ü®v·µ¢,v‚±º‚ü© = Œ¥·µ¢‚±º (Kronecker delta)</p>
<p><strong>Advantage:</strong> Easy coordinate computation: x = Œ£‚ü®x,v·µ¢‚ü©v·µ¢</p>
</div>

<div class="component-card">
<h4>Gram-Schmidt Process</h4>
<p>Converts linearly independent set {v‚ÇÅ,...,v‚Çñ} to orthonormal set {u‚ÇÅ,...,u‚Çñ}:</p>
<ol>
<li>u‚ÇÅ = v‚ÇÅ/‚Äñv‚ÇÅ‚Äñ</li>
<li>w‚ÇÇ = v‚ÇÇ - ‚ü®v‚ÇÇ,u‚ÇÅ‚ü©u‚ÇÅ, then u‚ÇÇ = w‚ÇÇ/‚Äñw‚ÇÇ‚Äñ</li>
<li>Continue: subtract projections onto previous vectors, then normalize</li>
</ol>
</div>

<div class="component-card">
<h4>Orthogonal Projection</h4>
<p>Projection of v onto subspace W with orthonormal basis {u‚ÇÅ,...,u‚Çñ}:</p>
<div class="formula">proj_W(v) = Œ£·µ¢‚ü®v,u·µ¢‚ü©u·µ¢</div>
<p><strong>Best Approximation Theorem:</strong> proj_W(v) is the closest vector in W to v.</p>
</div>
</div>
</div>

<div class="section">
<h2>üî¨ Advanced Topics</h2>

<div class="theory-box">
<h4>Singular Value Decomposition (SVD)</h4>
<p>Any m√ón matrix A can be factored as:</p>
<div class="formula">A = UŒ£V^T</div>
<p>where U (m√óm) and V (n√ón) are orthogonal, and Œ£ is m√ón diagonal with non-negative singular values œÉ‚ÇÅ ‚â• œÉ‚ÇÇ ‚â• ... ‚â• 0.</p>
<p><strong>Applications:</strong></p>
<ul>
<li>Data compression and dimensionality reduction</li>
<li>Principal Component Analysis (PCA)</li>
<li>Pseudoinverse computation</li>
<li>Low-rank approximations</li>
</ul>
</div>

<div class="theory-box">
<h4>Jordan Canonical Form</h4>
<p>Every matrix A over ‚ÑÇ is similar to a Jordan matrix J:</p>
<div class="formula">A = PJP^(-1)</div>
<p>where J consists of Jordan blocks for each eigenvalue. This generalizes diagonalization for matrices 
that lack sufficient eigenvectors.</p>
</div>
</div>

<div class="section">
<h2>‚ö° Computational Methods</h2>

<div class="component-grid">
<div class="component-card">
<h4>Gaussian Elimination</h4>
<p>Systematic row operations to achieve row echelon form:</p>
<ol>
<li>Forward elimination: create zeros below pivots</li>
<li>Back substitution: solve from bottom row up</li>
</ol>
<p><strong>Complexity:</strong> O(n¬≥) for n√ón system</p>
</div>

<div class="component-card">
<h4>LU Decomposition</h4>
<div class="formula">A = LU</div>
<p>L is lower triangular, U is upper triangular. Efficient for solving multiple systems Ax = b with same A but different b.</p>
</div>

<div class="component-card">
<h4>QR Decomposition</h4>
<div class="formula">A = QR</div>
<p>Q is orthogonal, R is upper triangular. Numerically stable method for least squares problems and eigenvalue computation.</p>
</div>

<div class="component-card">
<h4>Power Method</h4>
<p>Iterative algorithm to find dominant eigenvalue:</p>
<ol>
<li>Start with random vector x‚ÇÄ</li>
<li>Iterate: x_{k+1} = Ax_k/‚ÄñAx_k‚Äñ</li>
<li>Converges to eigenvector of largest |Œª|</li>
</ol>
</div>
</div>
</div>

<div class="section">
<h2>üí° Applications in Physics and Engineering</h2>

<div class="component-grid">
<div class="component-card">
<h4>Quantum Mechanics</h4>
<p>State vectors in Hilbert space, operators as matrices, eigenvalues as observable measurements. 
The Hamiltonian operator's eigenvectors are energy eigenstates.</p>
</div>

<div class="component-card">
<h4>Differential Equations</h4>
<p>System x' = Ax solved via eigenvalues: general solution is linear combination of e^(Œªt)v where (Œª,v) 
are eigenvalue-eigenvector pairs of A.</p>
</div>

<div class="component-card">
<h4>Computer Graphics</h4>
<p>Rotations, scaling, shearing represented as matrices. Composite transformations via matrix multiplication. 
Homogeneous coordinates enable translations as matrix operations.</p>
</div>

<div class="component-card">
<h4>Machine Learning</h4>
<p>PCA for dimensionality reduction, least squares for linear regression, gradient descent updates 
involve matrix-vector products, neural network layers are linear transformations.</p>
</div>
</div>
</div>
